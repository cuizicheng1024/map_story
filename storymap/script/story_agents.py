"""
èŒè´£ï¼šè´Ÿè´£â€œæ•…äº‹ç”Ÿæˆâ€ï¼ˆè°ƒç”¨ LLMï¼‰ï¼Œä¸åŒ…å«åœ°å›¾æˆ–è·ç¦»ç›¸å…³é€»è¾‘ã€‚
æç¤ºè¯ä» docs/ ç›®å½•åŠ è½½ï¼Œä¾¿äºé›†ä¸­ç®¡ç†ä¸è°ƒä¼˜ã€‚
"""
import argparse
import json
import os
import re
import requests
import urllib3
from typing import Dict, List, Optional

from dotenv import load_dotenv

# ç¦ç”¨ urllib3 çš„ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def _project_root() -> str:
    return os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

local_env = os.path.join(os.path.dirname(__file__), ".env")
load_dotenv(dotenv_path=local_env)

_MAX_TEXT_LEN = 200

def _validate_person(text: object) -> Optional[str]:
    if not isinstance(text, str):
        return "è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
    cleaned = text.strip()
    if not cleaned:
        return "è¾“å…¥ä¸èƒ½ä¸ºç©º"
    if len(cleaned) > _MAX_TEXT_LEN:
        return f"è¾“å…¥è¿‡é•¿ï¼ˆæœ€å¤š {_MAX_TEXT_LEN} å­—ç¬¦ï¼‰"
    return None

class StoryAgentLLM:
    """
    ä¸»è¦èŒè´£ï¼š
    - ç»Ÿä¸€ç®¡ç†æ¨¡å‹ IDã€API Keyã€Base URL ç­‰åŸºç¡€é…ç½®
    - è°ƒç”¨ Qveris çš„ Execute Tool æ¥å£æ¥æ‰§è¡Œå¤§æ¨¡å‹å¯¹è¯
    - å…¼å®¹ OpenAI æ ¼å¼çš„ messages è¾“å…¥
    """
    def __init__(
        self,
        model: Optional[str] = None,
        apiKey: Optional[str] = None,
        baseUrl: Optional[str] = None,
        timeout: Optional[int] = None,
        event_callback: Optional[callable] = None,
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚
        ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼›å¦‚æœæŸä¸ªå‚æ•°ä¸º Noneï¼Œåˆ™ä¼šå›é€€åˆ°ç¯å¢ƒå˜é‡ï¼š
        - LLM_MODEL_ID  -> æ¨¡å‹ ID
        - LLM_API_KEY   -> Qveris API Key
        - LLM_BASE_URL  -> Qveris API Base URL (ä¾‹å¦‚ https://qveris.ai/api/v1)
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        self.event_callback = event_callback
        self.apiKey = apiKey or os.getenv("LLM_API_KEY")
        self.baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        # Increase default timeout to 300 seconds (5 minutes)
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", "300"))
        
        # Qveris Tool ID for ZHIPU GLM-4 chat completions
        self.tool_id = "bigmodel.chat.completions.create.v4.bbf1f5ab"

        if not self.model or not self.apiKey or not self.baseUrl:
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

    def _emit(self, message: str) -> None:
        if not self.event_callback:
            return
        try:
            self.event_callback(message)
        except Exception:
            pass

    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> Optional[str]:
        """
        é€šè¿‡ Qveris Execute Tool æ¥å£è°ƒç”¨å¤§æ¨¡å‹ã€‚
        """
        import time
        max_retries = 3
        
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹ (via Qveris)...")
        self._emit(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹ (via Qveris)...")

        url = f"{self.baseUrl.rstrip('/')}/tools/execute"
        headers = {
            "Authorization": f"Bearer {self.apiKey}",
            "Content-Type": "application/json"
        }
        
        # æ„é€ ä¼ é€’ç»™å·¥å…·çš„å‚æ•°
        params_to_tool = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.apiKey}" 
        }

        payload = {
            "tool_id": self.tool_id,
            "parameters": params_to_tool
        }

        last_error = None
        for attempt in range(1, max_retries + 1):
            try:
                # Qveris execute tool æ¥å£é€šå¸¸ä¸æ”¯æŒæµå¼è¿”å›ï¼Œè¿™é‡Œä½¿ç”¨åŒæ­¥è°ƒç”¨
                # ç¦ç”¨ SSL éªŒè¯ä»¥è§£å†³è¯ä¹¦é”™è¯¯
                resp = requests.post(url, headers=headers, json=payload, timeout=self.timeout, verify=False)
                resp.raise_for_status()
                
                data = resp.json()
                
                if not data.get("success"):
                    error_msg = data.get("error_message") or "Unknown error"
                    raise RuntimeError(f"Qveris execution failed: {error_msg}")

                tool_result = data.get("result", {}).get("data", {})
                
                # è§£æ OpenAI æ ¼å¼çš„å“åº”
                content = ""
                if isinstance(tool_result, dict):
                    choices = tool_result.get("choices", [])
                    if choices and len(choices) > 0:
                        message = choices[0].get("message", {})
                        content = message.get("content", "")
                
                # å¦‚æœ result.data ç›´æ¥æ˜¯å­—ç¬¦ä¸²ï¼ˆæŸäº›å·¥å…·å¯èƒ½ç›´æ¥è¿”å›å†…å®¹ï¼‰
                if not content and isinstance(tool_result, str):
                    content = tool_result

                if content:
                    print(content)
                    self._emit(f"âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ")
                    return content
                else:
                    print("âš ï¸ æ¨¡å‹è¿”å›å†…å®¹ä¸ºç©º")
                    # ç©ºå†…å®¹ä¸è§†ä¸ºé”™è¯¯ï¼Œç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²
                    return ""

            except Exception as e:
                last_error = e
                print(f"âš ï¸ ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries:
                    wait_time = 2 * attempt  # ç®€å•çš„æŒ‡æ•°é€€é¿
                    print(f"â³ {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ è°ƒç”¨LLM APIæœ€ç»ˆå¤±è´¥: {e}")
                    self._emit(f"âŒ è°ƒç”¨LLM APIæœ€ç»ˆå¤±è´¥: {e}")
        
        return None


def _read_prompt(relpath: str) -> str:
    """
    è¯»å– docs/ ç›®å½•ä¸‹çš„æç¤ºè¯æ–‡ä»¶å†…å®¹ã€‚
    """
    root = os.path.dirname(os.path.abspath(__file__))
    # script/../docs -> storymap/docs
    prompt_path = os.path.join(root, "..", "docs", relpath)
    if not os.path.exists(prompt_path):
         # Fallback for when running from project root
         root_proj = _project_root()
         prompt_path = os.path.join(root_proj, "storymap", "docs", relpath)
    
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()


def generate_historical_markdown(llm: "StoryAgentLLM", person: str) -> Optional[str]:
    """
    ç”ŸæˆæŒ‡å®šäººç‰©çš„ç”Ÿå¹³ Markdownã€‚
    """
    system_prompt = _read_prompt("story_system_prompt.md")
    user_prompt = f"è¯·æ•´ç†å†å²äººç‰©ã€Œ{person}ã€çš„ç”Ÿå¹³ä¿¡æ¯ï¼Œå¹¶æŒ‰è¦æ±‚è¾“å‡ºã€‚"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    return llm.think(messages, temperature=0.1)


def extract_historical_figures(llm: "StoryAgentLLM", text: str) -> List[str]:
    """
    ä»è¾“å…¥æ–‡æœ¬ä¸­æŠ½å–å†å²äººç‰©åç§°åˆ—è¡¨ã€‚
    """
    if not isinstance(text, str):
        return []
    sys_prompt = _read_prompt("extract_names_prompt.md")
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": text},
    ]
    raw = llm.think(messages, temperature=0)
    if not raw:
        return []
    try:
        data = json.loads(raw.strip())
        if isinstance(data, list):
            names = [str(x).strip() for x in data if str(x).strip()]
            return list(dict.fromkeys(names))
    except Exception:
        pass
    cleaned = raw.strip()
    return [cleaned] if cleaned else []


def save_markdown(person: str, content: str) -> str:
    """
    ä¿å­˜ Markdown åˆ° examples/story/ ç›®å½•ï¼Œè‹¥å­˜åœ¨åˆ™è¦†ç›–ã€‚
    """
    root = _project_root()
    base = os.path.join(root, "storymap", "examples", "story")
    os.makedirs(base, exist_ok=True)
    filename = f"{person}.md"
    path = os.path.join(base, filename)
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"âœ… äººç‰©ç”Ÿå¹³å·²ä¿å­˜: {path}")
    return path


def run_interactive(llm: "StoryAgentLLM") -> None:
    """
    äº¤äº’å¼è¾“å…¥äººç‰©å¹¶ç”Ÿæˆ Markdownã€‚
    """
    while True:
        try:
            name = input("è¯·è¾“å…¥å†å²äººç‰©ï¼ˆq/quit/exit é€€å‡ºï¼‰ï¼š").strip()
        except EOFError:
            break
        if not name:
            continue
        err = _validate_person(name)
        if err:
            print(err)
            continue
        if name.lower() in {"q", "quit", "exit"}:
            print("å·²é€€å‡ºã€‚")
            break
        targets = extract_historical_figures(llm, name)
        if not targets:
            print("æœªè¯†åˆ«åˆ°å†å²äººç‰©ï¼Œè¯·é‡è¯•ã€‚")
            continue
        for person in targets:
            md = generate_historical_markdown(llm, person)
            if md:
                saved = save_markdown(person, md)
                print(f"å·²ç”Ÿæˆï¼š{saved}")
                print(md)
            else:
                print(f"æœªå–å¾—ã€Œ{person}ã€ç»“æœã€‚")


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäºç¯å¢ƒå˜é‡é…ç½®çš„ LLMï¼Œç”Ÿæˆå†å²äººç‰©çš„ Markdown ç”Ÿå¹³ä¿¡æ¯ã€‚"
    )
    parser.add_argument(
        "-p", "--person", help="å†å²äººç‰©å§“åï¼Œä¾‹å¦‚ï¼šæç™½ã€æœç”«ã€è¯¸è‘›äº®", required=False
    )
    args = parser.parse_args()

    if args.person:
        try:
            err = _validate_person(args.person)
            if err:
                print(err)
                return
            client = StoryAgentLLM()
            targets = extract_historical_figures(client, args.person)
            if not targets:
                print("æœªè¯†åˆ«åˆ°å†å²äººç‰©ã€‚")
                return
            for person in targets:
                md = generate_historical_markdown(client, person)
                if md:
                    saved = save_markdown(person, md)
                    print(f"å·²ç”Ÿæˆï¼š{saved}")
                    print(md)
        except ValueError as e:
            print(e)
        return

    try:
        client = StoryAgentLLM()
        run_interactive(client)
    except ValueError as e:
        print(e)


if __name__ == "__main__":
    main()
